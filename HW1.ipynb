{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STA 208: Homework 1\n",
    "This is based on the material in Chapters 2, 3 of 'Elements of Statistical Learning' (ESL), in addition to lectures 1-4.  Chunzhe Zhang came up with the dataset and the analysis in the second section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "We use a script that extracts your answers by looking for cells in between the cells containing the exercise statements (beginning with __Exercise X.X__).  So you \n",
    "\n",
    "- MUST add cells in between the exercise statements and add answers within them and\n",
    "- MUST NOT modify the existing cells, particularly not the problem statement\n",
    "\n",
    "To make markdown, please switch the cell type to markdown (from code) - you can hit 'm' when you are in command mode - and use the markdown language.  For a brief tutorial see: https://daringfireball.net/projects/markdown/syntax\n",
    "\n",
    "In the conceptual exercises you should provide an explanation, with math when necessary, for any answers.  When answering with math you should use basic LaTeX, as in \n",
    "$$E(Y|X=x) = \\int_{\\mathcal{Y}} f_{Y|X}(y|x) dy = \\int_{\\mathcal{Y}} \\frac{f_{Y,X}(y,x)}{f_{X}(x)} dy$$\n",
    "for displayed equations, and $R_{i,j} = 2^{-|i-j|}$ for inline equations.  (To see the contents of this cell in markdown, double click on it or hit Enter in escape mode.)  To see a list of latex math symbols see here: http://web.ift.uib.no/Teori/KURS/WRK/TeX/symALL.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Conceptual Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 1.1.__ (5 pts) Recall that the Hamming loss for Binary classification ($y \\in \\{0,1\\}$) is \n",
    "$$l(y,\\hat y) = 1\\{y \\ne \\hat y\\} = (y - \\hat y)^2$$\n",
    "as long as $\\hat y \\in \\{0,1\\}$.\n",
    "This loss can be extended to multiclass classification where there are $K$ possible values that $y$ can take (for example 'dog','cat','squirrel' or 1-5 stars).  Explain how you can re-encode $y$ and $\\hat y$ to be a $K-1$ dimensional vector that generalizes binary classification, and rewrite the loss using vector operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STA 208 HW1 Tongbi Tu 998727547\n",
    "# 1.1 \n",
    "\n",
    "For $K$ possible classes, $y$ and $\\hat y$ can be defined through $K$ dimensional vector that generalizes binary classification as follows: $$y=(y_{1},...,y_{K})^{T},\\hat y=(y_{1},...,y_{K})^{T}$$\n",
    "where $y_{i} \\in \\{0,1\\},i=1,...,K$; $\\sum_{k=1}^{n}y_{i}=1$;\n",
    "$\\hat y_{i} \\in \\{0,1\\},i=1,...,K$; $\\sum_{k=1}^{n}\\hat y_{i}=1$\n",
    "\n",
    "Then the associated loss using vector operations can be written as:\n",
    "$$l(y,\\hat{y})=(y-\\hat{y})^{T}(y-\\hat{y})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 1.2__ (5 pts) Ex. 2.7 in ESL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2\n",
    "\n",
    "## (a)\n",
    "\n",
    "Based on the definition of $k$-nearest-neighbor, the weights for $k$-nearest-neighbor regression can be obtained as:$$\n",
    "l_{i}(x_{0};\\chi)=\\left\\{\n",
    "\\begin{array}{ll}\n",
    "\\frac{1}{k}&x_{0}\\text{ is one of the nearest $k$ points}\\\\\n",
    "0&\\text{otherwise}\n",
    "\\end{array}\n",
    " \\right.\n",
    "$$\n",
    "For linear regression,take the least squares estimators as an exmaple here. Then the prediction for a given $x_{0}$ can be calculated as: $$\\hat{f}(x_{0})=x_{0}^{T}\\hat{\\beta}=x_{0}^{T}(X^{T}X)^{-1}X^{T}y$$ So the corresponding weights can be obtained as:$$l_{i}(x_{0};\\chi)=x_{0}^{T}(X^{T}X)^{-1}X^{T}$$\n",
    "\n",
    "## (b)\n",
    "\n",
    "$$E_{y\\mid{x}}(f(x_{0})-\\hat{f(x_{0})})^{2}=E_{y\\mid{x}}(f(x_{0})^{2}-2f(x_{0})\\hat{f(x_{0})}+\\hat{f(x_{0})}^{2})=(f(x_{0})-E_{y\\mid{x}}(\\hat{f(x_{0}})))^{2}+(E_{y\\mid{x}}\\hat{f(x_{0})}^{2}-(E_{y\\mid{x}}\\hat{f(x_{0})})^2)=(bias_{y\\mid{x}})^{2}+Var_{y\\mid{x}}(\\hat{f(x_{0}}))$$\n",
    "\n",
    "## (c)\n",
    "\n",
    "$$E_{y,x}(f(x_{0})-\\hat{f(x_{0})})^{2}=E_{y,x}(f(x_{0})^{2}-2f(x_{0})\\hat{f(x_{0})}+\\hat{f(x_{0})}^{2})=(f(x_{0})-E_{y,x}(\\hat{f(x_{0}})))^{2}+(E_{y,x}\\hat{f(x_{0})}^{2}-(E_{y,x}\\hat{f(x_{0})})^2)=(bias_{y,x})^{2}+Var_{y,x}(\\hat{f(x_{0}}))$$\n",
    "\n",
    "## (d)\n",
    "\n",
    "Based on Adam's Law and the Law of total variance, \n",
    "$$E_{Y,X}f(\\hat{x_0})=E(E_{Y\\mid{X}}f(\\hat{x_0})$$\n",
    "$$Var_{Y,X}f(\\hat{x_0})=E[Var_{Y\\mid{X}}f(\\hat{x_0}]+Var(E_{Y\\mid{X}}f(\\hat{x_0}))$$\n",
    "So the relationship between (b) and (c) can be established through the two equations above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 1.3__ (5 pts, 1 for each part) Recall that the true risk for a prediction function, $f$, a loss function, $\\ell$, and a joint distribution for $Y,X$ is \n",
    "$$R(f) = E \\ell(y,f(x))$$\n",
    "For a training set $\\{x_i,y_x\\}_{i=1}^n$, the empirical risk is \n",
    "$$R_n = \\frac{1}{n} \\sum_{i=1}^n \\ell(y_i,f(x_i)).$$\n",
    "Let $y = x^\\top \\beta + \\epsilon$ be a linear model for $Y|X$, where $x,\\beta$ are $p$-dimensional such that $\\epsilon$ is Gaussian with mean 0 and variance $\\sigma^2$ (independent of X).\n",
    "Let $\\ell(y,\\hat y) = (y - \\hat y)^2$ be square error loss.\n",
    "\n",
    "1. Show that $f^\\star(x) = x^\\top \\beta$ gives the smallest true risk (also known as the Bayes rule).\n",
    "1. Why can't we use this prediction in practice?\n",
    "1. Recall that OLS is the empirical risk minimizer for linear functions.  Why does this tell us the following:\n",
    "$$ E R_n (\\hat f) \\le R(f^\\star)$$\n",
    "1. How do we know that $E R_n (\\hat f) \\le R(\\hat f)$? and use this to answer Ex. 2.9 in ESL.  \n",
    "1. What about this was specific to OLS and least squares loss (can this be generalized)?  What is the most general statement that you can think of that you can prove in this way?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3\n",
    "\n",
    "## (1)\n",
    "\n",
    "$$\\underset{\\hat{y}}{\\operatorname{argmin}}E((y-\\hat{y})\\mid{X})^2=E(y\\mid{X})=E(x^{T}\\beta+\\epsilon)=x^{T}\\beta=f^{*}(x)$$\n",
    "So $f^{*}(x)=x^{T}\\beta$ gives the smallest true risk.\n",
    "\n",
    "## (2)\n",
    "\n",
    "As in practice, the whole dataset is unknown and only sample dataset can be obtained. So the true {\\beta} cannot be obtained. Thus, $f^{*}(x)=x^{T}\\beta$ cannot be used in practice.\n",
    "\n",
    "## (3)\n",
    "OLS is the empirical risk minimizer for linear function, the corresponding empirical risk is:\n",
    "$$ER_{n}(\\hat{f})=E(\\frac{1}{n}\\sum_{i=1}^{n}(y-\\hat{f})^2)=\\frac{1}{n}n\\sigma^2=\\sigma^2$$\n",
    "$$R(f^*)=E(y-\\hat{y})^2=E(x_{0}^{T}\\beta+\\epsilon_{0}-x_{0}^T\\beta)^2$$\n",
    "$$E(\\epsilon_{0}^2)=\\sigma^2$$\n",
    "\n",
    "Therefore,$ER_{n}(\\hat{f})\\le R(f^*)$ is obtained.\n",
    "\n",
    "## (4)\n",
    "\n",
    "$$R(\\hat{f})=E(y-\\hat{y})^2=E_{y_0\\mid{x_0}}E_{T}(y_0-\\hat{y_{0}})^2=Var(y_0\\mid{x_0})+Var_T(\\hat{y_0})+Bias^2(\\hat{y_0})=\\sigma^2+E_{T}x^{T}_{0}(X^TX)^{-1}x_{0}\\sigma^2+0^2\\ge\\sigma^2$$\n",
    "\n",
    "Or the prove can be completed as: from (1) we get that $R(f^*)$ denotes the smalllest true risk; from (3) we have $ER_{n}(\\hat{f})\\le R(f^*)$. Then $ER_{n}(\\hat{f})\\le R(f^*)\\le R(\\hat{f})$.\n",
    "\n",
    "So this proves Ex. 2.9, i.e.. the expected empirical risk $ER_{n}(\\hat{f})=E[R_{tr}(\\hat{\\beta})]$is no greater than the expected prediction error $R(\\hat{f})=E[R_{te}(\\hat{\\beta})]$.\n",
    "\n",
    "## (5)\n",
    "\n",
    "For OLS and least squares loss,the empirical risk is no larger than the expected prediction error. To be more general,this can be related to the Gauss-Markov theorem, i.e., the least squares estimate has variance no bigger than that of any other liner unbiased estimate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 1.4__ (5 pts) Ex. 3.5 in ESL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4\n",
    "From the ridge regression equation (3.41) $$\\hat{\\beta^{ridge}}=\\underset{\\beta}{\\operatorname{argmin}}[\\sum_{i=1}^{N}(y_{i}-\\beta_{0}-\\sum_{j=1}^{p}x_{ij}\\beta_{j})^2+\\lambda\\sum_{j=1}^p\\beta_{j}^2]$$\n",
    "\n",
    "Take $$L_1=\\sum_{i=1}^{N}(y_{i}-\\beta_{0}-\\sum_{j=1}^{p}x_{ij}\\beta_{j})^2+\\lambda\\sum_{j=1}^p\\beta_{j}^2$$Then\n",
    "$$L_1=\\sum_{i=1}^{N}(y_{i}-\\beta_{0}-\\sum_{j=1}^{p}(x_{ij}+(\\bar{x_j}-\\bar{x_j}))\n",
    "\\beta_{j})^2+\\lambda\\sum_{j=1}^p\\beta_{j}^2=\\sum_{i=1}^{N}(y_{i}-\\beta_{0}-\\sum_{j=1}^{p}\\bar{x_j}\\beta_{j}-\\sum_{j=1}^{p}(x_{ij}-\\bar{x_j})\\beta_{j})^2+\\lambda\\sum_{j=1}^p\\beta_{j}^2$$\n",
    "\n",
    "Next, take $\\beta_{0}^{c}=\\beta_{0}+\\sum_{j=1}^{p}\\bar{x_j}\\beta_{j};\\beta_{j}^c=\\beta_{i},i=1,\\dots,p$ Then $L_1$ can be written as:$$L_2=\\sum_{i=1}^{N}\\big(y_i-\\beta_{0}^c-\\sum_{j=1}^p(x_{ij}-\\bar{x_j})\\beta_{j}^c\\big)^2+\\lambda\\sum_{j=1}^p{\\beta_{j}^c}^2$$\n",
    "So if $\\beta_i$ minimize $L_1$, $\\beta_j^c$will also minimize $L_2$. Therefore, the equivalence is established. So compared with the solutions to the original equation (3.41), $\\beta_{j}^c,j=1,\\dots,p$ of the solution to the modified equation remained unchanged and only  $\\beta_{0}^c$ shifted from the original $\\beta_{0}$,i.e., the solution does not depend on the measurement scale.\n",
    "\n",
    "For the Lasso regression,\n",
    "\n",
    " $$\\hat{\\beta^{lasso}}=\\underset{\\beta}{\\operatorname{argmin}}[\\sum_{i=1}^{N}(y_{i}-\\beta_{0}-\\sum_{j=1}^{p}x_{ij}\\beta_{j})^2]$$\n",
    " Similarily,\n",
    " $$\\hat{\\beta_c^{lasso}}=\\underset{\\beta_c}{\\operatorname{argmin}}[\\sum_{i=1}^{N}(y_{i}-\\beta_{0}^c-\\sum_{j=1}^{p}(x_{ij}-\\bar{x}_j)\\beta_{j})^2]$$\n",
    " The equivalence between the modified equation and the original equation for lasso regression is formulated. So a similar result holds for the lasso.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 1.5__ (5 pts) Ex 3.9 in ESL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5\n",
    "\n",
    "An efficient procedure for implementing the Forward stepwise regression can be described as follows:\n",
    "\n",
    "To pick the next candidate predictor $x_j$ from the $p-q$ possible predictors, the selected predictor should be the one that had the largest projection in the direction of $\\gamma$. The new vector $x_{j'}$ where $$x_{j'}=\\underset{j=q+1,\\dots,p}{\\operatorname{argmin}}\\big|<\\frac {x_q}{||x_q||},\\gamma>\\big|$$\n",
    "This way can select the predictor that explains the maximal amount of variance in $\\gamma$ when $X_1$ is included. Consequently, the residual sum of squares is reduced to the most. Then repeat the above procedure by updating $X_2$ based on Algorithm 3.1 that is reported in the book on Page 54, until the desired number of predictors is achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "You will be graded based on several criteria, and each is on a 5 point scale (5 is excellent - A - 1 is poor - C - 0 is not answered - D/F).  You should strive to 'impress us' if you want a 5.  This means excellent code, well explained conclusions, well annotated plots, correct answers, etc.\n",
    "\n",
    "We will be grading you on several criteria:\n",
    "\n",
    "- Conclusions: Conclusions should be consistent with the evidence provided, the conclusion should be well justified, the principles of machine learning that you have learned should be respected (such as overfitting and underfitting etc.)\n",
    "- Correctness of calculations: code should be correct and reflect the principles learned in this course, the logic should be sound, the methods should match the setting and context, you should try many applicable methods that you have learned as long as they apply. \n",
    "- Code, Figures, and Text: Code should be annotated and easy to follow, with docstrings on the functions; captions, titles,  for figures "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 2__ You should run the following code cells to import the code and reduce the variable set.  Address the questions after the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn import linear_model, neighbors\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# dataset path\n",
    "data_dir = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quality</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>68.5</td>\n",
       "      <td>0.99620</td>\n",
       "      <td>3.26</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>110.0</td>\n",
       "      <td>0.99630</td>\n",
       "      <td>3.25</td>\n",
       "      <td>9.2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71.5</td>\n",
       "      <td>0.99551</td>\n",
       "      <td>3.56</td>\n",
       "      <td>10.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>110.0</td>\n",
       "      <td>0.99600</td>\n",
       "      <td>3.28</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   quality  density    pH  alcohol  time\n",
       "0     90.0  0.99780  3.51      9.4     5\n",
       "1     68.5  0.99620  3.26     10.0     4\n",
       "2    110.0  0.99630  3.25      9.2     5\n",
       "3     71.5  0.99551  3.56     10.8     5\n",
       "4    110.0  0.99600  3.28      9.8     6"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data = pd.read_csv(data_dir+\"/hw1.csv\", delimiter=',')\n",
    "sample_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response variable is quality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.array(sample_data.iloc[:,range(1,5)])\n",
    "y = np.array(sample_data.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loo_risk(X,y,regmod):\n",
    "    \"\"\"\n",
    "    Construct the leave-one-out square error risk for a regression model\n",
    "    \n",
    "    Input: design matrix, X, response vector, y, a regression model, regmod\n",
    "    Output: scalar LOO risk\n",
    "    \"\"\"\n",
    "    loo = LeaveOneOut()\n",
    "    loo_losses = []\n",
    "    for train_index, test_index in loo.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        regmod.fit(X_train,y_train)\n",
    "        y_hat = regmod.predict(X_test)\n",
    "        loss = np.sum((y_hat - y_test)**2)\n",
    "        loo_losses.append(loss)\n",
    "    return np.mean(loo_losses)\n",
    "\n",
    "def emp_risk(X,y,regmod):\n",
    "    \"\"\"\n",
    "    Return the empirical risk for square error loss\n",
    "    \n",
    "    Input: design matrix, X, response vector, y, a regression model, regmod\n",
    "    Output: scalar empirical risk\n",
    "    \"\"\"\n",
    "    regmod.fit(X,y)\n",
    "    y_hat = regmod.predict(X)\n",
    "    return np.mean((y_hat - y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 2.1__ (5 pts) Compare the leave-one-out risk with the empirical risk for linear regression, on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOO Risk: 243.53350804\n",
      "Emp Risk: 242.27402643\n"
     ]
    }
   ],
   "source": [
    "## 2.1\n",
    "lin1 = linear_model.LinearRegression(fit_intercept=False)\n",
    "print('LOO Risk: '+ str(loo_risk(X,y,lin1)))\n",
    "print('Emp Risk: ' + str(emp_risk(X,y,lin1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results indicate that the leave-one-out risk is slightly larger than the empirical risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 2.2__ (10 pts) Perform kNN regression and compare the leave-one-out risk with the empirical risk for k from 1 to 50.  Remark on the tradeoff between bias and variance for this dataset and compare against linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEaCAYAAAAfVJzKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl8U1X+//HXzdp9CS1UyiYUZF9b2SlCEQdQEREF0WFx\nRr8gsijCuIHigqyKoujogMr4G9GBui/UahFwpFI2WYQqUAqF0jaU7mmS+/sjbaDQna7J5/l45JHk\n5ubec1J45+Tcc89VVFVVEUII4bI09V0AIYQQtUuCXgghXJwEvRBCuDgJeiGEcHES9EII4eIk6IUQ\nwsVJ0LuBoUOH8sADD1Rq3RMnTqAoCtu3b6/RMnz88ce0a9cOrVbLlClTanTbtVVmUbY2bdrw/PPP\n13cxRCVJ0DdiU6ZMQVEUFEVBq9XSokUL7r//fk6fPl1ivc2bN7Nq1ap6KiXYbDamTZvGhAkTSEpK\n4tVXX63zMhR/Vo8//niJ5cnJySiKwo8//uhcVvx5HjhwoMS6zz//PG3atCl3P0OHDnX+TfR6PW3a\ntGHWrFlcuHChpqrSIMTHxzN37tz6LoaoJAn6Rm7w4MGkpKSQlJTEhx9+yJ49e7jrrrtKrGMymfDz\n86unEkJKSgrZ2dmMGjWK0NBQ/P39q7WdwsLCayqHh4cHa9as4eTJkxWuazAYmD9/frX2M2nSJFJS\nUjh+/Djr1q1j8+bNzJgxo1rbqiqLxVIn+wkODsbb27tO9iWunQR9I2cwGAgJCSE0NJQhQ4bw97//\nnZ9//pmLFy8617my62b79u0MHDgQX19ffH196dGjB99++22Z+1i6dCkmk4lt27ZV+f0bNmygZcuW\nAAwZMqRE6/mrr76iT58+GI1GmjZtyowZM8jJyXG+d8qUKURFRfHaa6/Rpk0bjEYjeXl5lfpcriwz\nwIABA+jRowdPPPFEhe9/5JFH2Lp1K1u3bq3U/i7n6elJSEgILVq04JZbbuGee+656vPJzs5m9uzZ\nhIaG4uXlRa9evdi8eXOJdfbs2UO/fv3w8PDghhtuYPPmzVd1mSiKwpo1a5g0aRL+/v7cd999AJw7\nd44pU6YQHByMr68vAwcOLPFZFBYWMm/ePFq0aIHRaOS6667jnnvucb5+8OBBRo4cSUBAAN7e3nTq\n1IkPPvjA+fqV5cjKyuLBBx8kODgYo9FIeHg43333nfP14u61TZs2MWbMGLy8vGjbti0bNmyo8ucr\nqk6C3oWcOXOGTz75BK1Wi1arLXUdq9XKbbfdRt++fUlISCAhIYHFixfj5eV11bp2u51Zs2bx2muv\nERcXx5AhQ6r0foC7776bXbt2AfDpp5+SkpLCgAED2L9/P7fddhtDhgxh3759vPfee3zxxRc89NBD\nJd6/a9cuYmNj+fTTT9m3bx8Gg6Hcz6C0MhdTFIUVK1bw//7f/+PXX38tdzvdunVjypQpzJ8/H7vd\nXu665UlMTOSrr74qUW5VVbn11lvZt28fH330Eb/99hv/93//xz333MP3338PQG5uLqNGjSI4OJhd\nu3bx3nvvsWLFClJTU6/ax7PPPsuAAQNISEjg+eefJy8vj5tuuomsrCy+/vpr9uzZw6hRoxgxYgSH\nDx8G4LXXXmPTpk1s3LiRY8eO8dlnn9GvXz/nNidOnEiTJk3YuXMnBw4cYNWqVQQGBpZZz2nTpvHt\nt9+yceNG9u7dy8CBAxkzZgxHjhwpsd7ChQu5//772b9/P/fccw8PPPAAR48erfbnKypJFY3WX//6\nV1Wr1are3t6qp6enCqiA+uijj5ZYLzIyUp0+fbqqqqqakZGhAuoPP/xQ6jaPHz+uAmpMTIw6fvx4\ntWPHjurJkyedr1f0/vK2+dNPPzmXTZ48WY2IiCixXnR0tKooinrixAln/fz9/dWsrKxKbb+sMhdv\na/jw4aqqqurYsWPVyMhIVVVV9dSpU1fVB1A/+OAD9fTp06qXl5f6r3/9S1VVVV2yZInaunXrcssS\nGRmp6nQ61dvbWzUajc6/yZo1a5zr/PDDD6rRaFQvXLhQ4r1Tp05Vb7/9dlVVVfXtt99Wvb29S6xz\n+PBhFVCXLFlSoqzTpk0rsZ3169eroaGhamFhYYnlN910kzp79mxVVVX1kUceUW+66SbVbreXWg8/\nPz91/fr1ZdazdevWznIcO3ZMBdQvv/yyxDq9evVSp06dqqrqpb/RypUrna9brVbVx8dHXbduXZn7\nETVDVw/fLaIG9e3bl/fee4/8/Hw2bdpETExMuaMhAgMDeeCBBxg5ciTDhg0jMjKSO+64gxtuuKHE\nelOnTsXLy4sdO3bQpEmTKr+/IgcPHmTYsGEllkVGRqKqKocOHaJ169YAdOrUCR8fn0pts6wyX+nl\nl1+mS5cufPbZZ/Tu3bvM9Zo3b86jjz7K008/XaJboyJ33HEHL774Ijk5Oaxdu5b09HRmzpzpfD0+\nPh6LxUJoaGiJ91ksFtq3bw/AoUOH6NSpU4njGR07diQgIOCq/d14440lnsfHx3P27Nmr1i0oKMDT\n0xNwfFYjRowgLCyMESNGMGLECG699VbnL4/HHnuMBx54gA0bNjB06FBuu+22Mj+rQ4cOAZT49VT8\n/Oeffy6xrGfPns7HWq2Wpk2bcu7cuVK3K2qOdN00cp6enoSFhdG1a1eee+45rr/+embNmlXue/75\nz3+ye/duRowYQVxcHF27duWtt94qsc7o0aM5fvw433zzTbXeX1OqcsCvvDJfrkOHDjz44IMsWLAA\nq9Va7rqPP/44NpuNlStXVrocfn5+hIWF0aNHD95++23OnTvHSy+95Hzdbrfj7+/P3r17S9wOHTrE\n119/7VxPUZRK7e/Kz8hut9OpU6ertn/48GH++c9/Ao7APX78OCtWrMBgMDB79mx69uzpPLbz9NNP\nc/ToUSZMmMBvv/1Gv379eOqppyr9GZTlyq43RVGuqWtMVI4EvYtZvHgx69evr7APumvXrsybN4+v\nv/6a6dOn8/bbb5d4/d577+W9995j2rRpvPfee1V+f0W6dOlS4uAgQFxcHIqi0KVLlyptq7Jlvtyi\nRYs4c+ZMheX28fHh2WefZdmyZdVueT777LO88MILzmGv4eHhXLhwgfz8fMLCwkrcWrVqBUDnzp05\nfPgwmZmZzu38/vvvlRqmGR4ezp9//un8wrn81rx58xJ1u+OOO1izZg2//vorhw8fJi4uzvl627Zt\nmTFjBp988gnPPfccb775Zqn7K/57Xfn33LZtG127dq3kpyRqkwS9i2nfvj233norTz75ZKmvJyYm\nsmDBArZv387Jkyf5+eef+emnn+jcufNV695zzz18+OGHPPjgg7zzzjtVfn955s+fT0JCAnPnzuXI\nkSN88803zJo1i3vvvdcZdtVRWplLExwczMKFC3nllVcq3Ob06dNp2bIl7777brXKNHz4cDp27Mhz\nzz0HwLBhw4iKimLcuHFER0fz559/snv3bl577TVni/vee+/Fx8fHeeDyl19+Yfr06Xh6elbY0r/3\n3nu5/vrrGT16NN999x0nTpzgl19+4aWXXiI6OhqA5cuX8+9//5uDBw9y/Phx/vWvf6HVaunQoQPZ\n2dnMnDmT2NhYjh8/zp49e/jmm2/K/Bu3a9eOu+66ixkzZvDtt99y5MgRZs+ezW+//VbtIaqiZknQ\nu6D58+fz3XfflTgJqJi3tzfHjh3jnnvuoUOHDtx5550MGDCA119/vdRt3XnnnWzatIlZs2bxxhtv\nVPn9ZenevTufffYZ27Zto0ePHtx3332MHj2adevWVafK5Za5LHPnziUoKKjC7Wm1WpYtW1bpoZ2l\neeyxx/jXv/5FYmIiiqLw2WefMW7cOObOnUvHjh0ZPXo0X375Je3atQPAy8uLr776inPnzhEREcHk\nyZOZPXs2Pj4+eHh4lLsvDw8P4uLiCA8PZ+rUqXTo0IFx48axa9cu57EPPz8/Vq1aRf/+/enWrRtb\ntmzhv//9LzfccAM6nQ6z2cz06dPp1KkTI0eOpFmzZnz44Ydl7vOdd95h5MiRTJ48mR49erBjxw6+\n+OILOnbsWO3PTNQcRVXlClNCNAYnT56kTZs2fPbZZ9x66631XRzRiEjQC9FAbdy4kdDQUK6//npO\nnjzJ448/zrlz5/j9998xGo31XTzRiMjwSiEaqPT0dBYtWsTp06cxmUwMHDiQjz/+WEJeVJm06IUQ\nwsXJwVghhHBxEvRCCOHiGkwf/ZkzZ8p9PSgoiLS0tDoqTcPhrvUG96271Nu9XEu9Lz8BrjzSohdC\nCBcnQS+EEC5Ogl4IIVxcg+mjF0K4PlVVyc/Px263XzVnz7lz5ygoKKinktWfiuqtqioajQYPD49K\nz2h6JQl6IUSdyc/PR6/Xo9NdHT06na7MK6O5ssrU22q1kp+f77yeQFVJ140Qos7Y7fZSQ16UT6fT\nXdO8/RL0Qog6U92uB3Ftn12jDnrt6dP4vvQSmqILOgghhLhaow56JTsb39dfx+OKK9sIIURZiq/L\ne6WNGzcyZMgQhgwZwujRo9m1a5fzNYvFwjPPPMPAgQMZOHAgU6dOrfAkz2KPPfYYR48eLfP1O+64\ng3379lWtElXUqIPe2qEDtqZNMWzfXt9FEUI0Ylu3bmXjxo1ER0ezbds2li5dysyZM0lNTQVg6dKl\n5OTksG3bNnbs2MEtt9zC3/72NyqaE9Jms7FixQo6dOhQF9UoU6MOehSFgkGDMO7YATIJpxCimt54\n4w2eeuopTCYTAN26deOuu+5iw4YN5OXl8dFHH7F48WLn6Ji7774bg8HA9lIame3bt+fZZ58lKiqK\n3bt3M378ePbt24fNZmPOnDkMGzaM4cOHX3W9Yrvdzpw5c3j55ZdrvH6N/vB3waBBeG3ejO7337HK\nZcuEaDT8nnkG/aFDzueKolTYQq5IYefOXCy6Nm9VHD16lO7du5dY1qNHDz7++GOOHz9OaGgovr6+\nJV7v3r07R48eZfDgwSWW5+bm0qtXLxYtWlRi+cGDBzl79iyxsbEAJS78brVaefjhh7nhhhuYPXt2\nlctfkcbdogcsgwYBYJTuGyFEA6DVahk9evRVy1u1akVSUhJPPfUUP/zwQ4kvjgULFtRayEMlWvQW\ni4VFixZhtVqx2Wz069ePCRMmkJ2dzerVqzl//jzBwcHMnTsXHx8fALZs2UJsbCwajYapU6fSs2fP\nWik8gC00FGubNhh/+omcBx6otf0IIWrWlS1vnU6H1Wqtl7K0b9+e/fv3M6io4Qiwf/9+OnToQJs2\nbTh9+jTZ2dnOjAM4cOAAUVFRV23LaDSWegJUQEAAW7du5ccff+SDDz7g888/Z9WqVQCEh4ezc+dO\nHnzwwQov/l4dFbbo9Xo9ixYtYvny5Sxbtoy9e/dy9OhRoqOj6datG2vWrKFbt25ER0cDkJyczM6d\nO1m1ahVPPvkk77777jUN9K+MgkGDMPzvf1BP/0iEEI3bjBkzePHFF8nIyADgt99+Y9OmTfz1r3/F\ny8uLu+66i2effRabzQbAxx9/TF5eXokvhopkZGRgt9sZPXo0jz/+OAcOHHC+NnHiRIYNG8ZDDz1U\nK192FQa9oijObxibzYbNZkNRFOLj44mMjAQgMjKS+Ph4AOLj4xkwYAB6vZ6mTZsSEhJCYmJijRf8\ncgWDB6PJzka/d2+t7kcI0fjl5eXRp08f5+2tt97i5ptv5u677+b2229nyJAhPP7447z22ms0a9YM\ngH/84x8YjUYGDx7MwIED+eKLL3jnnXeqdBJTSkoK48ePZ8SIEcyaNYt//OMfJV5/8MEH6dq1K488\n8kiNN44rdc1Yu93OggULOHv2LCNHjmTy5MlMmTKFDRs2AI5Jd6ZOncqGDRt49913ad++PUOGDAHg\nzTffpFevXvTr16/ENmNiYoiJiQEcQ5csFku5ZSj3Z116OvrQUGyLFmG/4sNr7Orz52x9c9e6u3K9\nz507Jxc3r6aCggLnF08xg8FQqfdWatSNRqNh+fLl5OTksGLFCpKSkkq8rihKlU/PjYqKKtG/VdEV\nViq6CktQly6o335L+t/+VqVyNHTuetUdcN+6u3K9CwoKypzAy5W/4MpT2XoXFBRc9e+iVq4w5e3t\nTZcuXdi7dy/+/v6YzWYAzGYzfn5+AJhMJtLT053vycjIcI5NrU2WQYMw7N6NkpdX6/sSQojGpMKg\nv3jxIjk5OYBjBM7+/fsJDQ0lPDycuLg4AOLi4oiIiAAuHT0uLCwkNTWVlJQUwsLCarEKDgWDBqFY\nLBguO21ZCCFEJbpuzGYza9euxW63o6oq/fv3p0+fPnTo0IHVq1cTGxvrHF4J0LJlS/r378+8efPQ\naDRMnz4djab2h+tb+vZF1esxbN9OQdFBYiGEEJU8GFsXKpogqDL9lk3uvBMlN5e0r7+uyaLVK1fu\nr62Iu9bdleudm5uLl5dXqa9JH335SvvsaqWPvqErGDQI/YEDKEXHDoQQQrhY0FsGDUJRVYw7d9Z3\nUYQQDVTLli0ZMWKE8/b666/X+j6XL1/OtnKmU58zZw5ffPFFre2/0U9qdjlLz57Yvb0xbt9Ofilz\nTQghhIeHB1u3bq2z/dlsNubPn19n+yuNS7Xo0eux9O0rE5wJIaqsb9++vPTSS4wYMYK//OUvHDhw\ngEmTJjFgwADef/99AHbu3Mm4ceO47777GDx4MAsWLCj1LNa+ffvywgsvMHLkSL744osSLfYXX3yR\noUOHEhUVxXOlzLS5bNky5syZ45xuoSa4VIseHP30HrGxaM6cwV7JAxVCiLr3zDN+HDqkdz6viWmK\nO3cu5LnnLpa7Tn5+PiNGjHA+f/jhh7n99tsBx8HNrVu3smjRIubOnUt0dDQFBQUMGzaM+++/H4C9\ne/fyww8/0KJFC+69916++uorxowZc9V+AgMD+fbbbwH44YcfAMd5RV9//TXbtm1DUZQSUxUDLFmy\nxDlhZE1eX9f1gr5obmjj9u3kTZhQz6URQjQ05XXd3HzzzQB06tSJ3NxcfHx88PHxwWAwOEO5Z8+e\ntG7dGoCxY8eya9euUoP+tttuu2qZn58fRqORRx999KrZAV555RV69+7NsmXLrrmOV3K5oLd27Iit\nSROMP/0kQS9EA3Zly7shDK8snodHUZQS88hoNBpnV8qVLe2yWt6lDSPV6XR8+eWXbN++nS+//JL1\n69ezZcsWwPEFsn//fsxmM4GBgTVSH2f5a3RrDYFGg2XgQLm8oBCiVuzdu5ekpCTsdjufffYZN954\nY6Xfm5OTQ1ZWFsOHD2fx4sUcuuwKW0OHDmXmzJncf//9ZGdn12iZXa5FD45+es/PPkOXmIi1jCu+\nCyHc05V99DfddBNPPPFEpd/fo0cPnnzySU6cOMGAAQP4y1/+Uun3ZmdnM23aNAoKClBV9arLDd56\n663k5OQwZcoUPvjgAzw9PSu97fK41JmxxbQnT9JswAAyn36anIceqoni1RtXPkuyIu5ad1eud2M/\nM3bnzp2sW7fOOQqnJsiZsdVka92aghtvxOeNN1CuOKothBDuxiWDHiBzyRI0GRn4Fl2TUQghrtXl\nY+obE5cNemvXruTeey/e69ejO3q0vosjhIBrHifvzq7ls3PZoAfIWrAA1ccH/6eflhE4QjQAGo2m\nwffDN0RWq/Wapnt3yVE3xewmExfnzyfgqafw+OYb8qtwdFwIUfM8PDzIz8+noKDgqvHnRqORgoKC\neipZ/amo3qqqotFo8PDwqPY+XDroAXLvuw/vjRvxe/ZZ8ocOhRoariSEqDpFUcocMujKo43KUxf1\ndumuGwB0OjKfew7dqVP4rFtX36URQog65/pBD1gGDiRvzBh8Xn8d7enT9V0cIYSoU24R9AAXn34a\nAL8lS+q5JEIIUbfcJuhtLVqQ/fDDeH7+OQa5ApUQwo24TdADZD/0ENaWLQmYPRuNdOEIIdyEWwU9\nnp5kvPMOmqwsmkyahCYjo75LJIQQtc69gh7HGbMZGzagS07GNHkySg1PByqEEA1NhePo09LSWLt2\nLRcuXEBRFKKiohg1ahSbNm3i+++/x8/PD4CJEyfSu3dvALZs2UJsbCwajYapU6fSs2fP2q1FFVn6\n9SPjzTcxPfAApmnTSP/gAyi64IAQQriaCoNeq9Vy33330bZtW/Ly8li4cCHdu3cHYPTo0VddLis5\nOZmdO3eyatUqzGYzS5Ys4dVXX72m03drQ8HNN3Nh1SoCZ88m8OGHMb/5Juhc/vwxIYQbqjB9AwMD\nadu2LQCenp6EhoaSUU7fdnx8PAMGDECv19O0aVNCQkJITEysuRLXoLzx48l89lk8v/oK/4ULZT4c\nIYRLqlITNjU1lePHjxMWFsaRI0f45ptv2LZtG23btuX+++/Hx8eHjIwM2l92VSeTyVTqF0NMTAwx\nMTEALF26lKCgoPILqtNVuE61LFyIraAA7xdfxEOvxz5hAmp4ONTwNRurq9bq3Qi4a92l3u6lLupd\n6aDPz89n5cqVTJkyBS8vL26++WbGjx8PwEcffcT777/PjBkzKr3jK6+AXtFcD7U6H8SMGfilpeHz\n9ttoi+aatl5/PZZevSjs2RNLz55YO3VCLePKOLXJXef/APetu9TbvVxLvSt7halKBb3VamXlypUM\nHjyYvn37AhAQEOB8ffjw4bz88suAowWfnp7ufC0jIwOTyVTpgtcLReHiokVkzZ2Lft8+DHv3ot+7\nF+OOHXht3gyAqijY2rShsFMnCjt3xtq5M4WdO2Nr0QLKuAq8EKKOqSpYLI7jbVpt3e/fZkPJyUHJ\nzkZTdO98XPw8NxdNdrbjeU4Omn794M47a7VYFQa9qqqsW7eO0NBQxowZ41xuNpsJLOre2LVrFy1b\ntgQgPDycNWvWMGbMGMxmMykpKYSFhdVS8WuW6ueHZfBgLIMHO5dpUlIw7NuH7vBh9IcOoT90CM+v\nvnK+bm3VioKhQ8m/6SYsAwag+vjUR9GFaJysVpS8PJS8PMjMRHf6NEpuriMMi+6dgZiVhZKVdenx\nZfdKVpYjPLOzUWw2AFSNBvR6VJ3OcW8woBqNqB4el26enqgeHmUPxLDbUQoLHeW8/L6wECU/H6Wg\nwHFffCssrHTV7d7eqN7e4OdX60Ff4cXBjxw5wjPPPEOrVq2c80dPnDiRHTt2cOLECRRFITg4mL//\n/e/O4N+8eTM//PADGo2GKVOm0KtXrwoLUpMXB69tSk4OuiNH0B84gHHbNozbt6PJyUHV67FERFAw\ndCjW669H9fJC9fLC7uXl+Efl5YW9SZMqDeVsSPWua+5a97qqt5KdjTYpCV1SEtqkJLSnTqHJzoaC\ngksBVnSPojhD8aqAtFgc61ksjgC0WC49LiUklfx8Z7grFkuly6sqCqqPD6q3N3ZfX8djHx/sV9yr\nnp6OlnXxPi0WFKv1UjgX3/LynI+x28v4kBTHF4VOh6rXX7rX6x2fw5VfHB4eqN7el8pZ/NjHxxns\nzjIWjUSsi66bCoO+rjSmoL+KxYLh118x/vgjHj/8gP7QoTJXVbVarGFhFHbqhLVTJ0dXUKdO2K+7\nrtQuoAZd71rm8nW329Gkp6M5dw5t0U2Tmoo3kJeZ6QjB4hAtLERVFChulRqNjscGg6OLwmp1tD6t\nVrDZHEFntTpCrDi4i8M7JwdtcjLaKwZJ2H18sPv7Q9H2ncFlMICqXhWOSl4eitXqWLe4LMXluzwU\nr7hXPT2vvnl44BMSwkWbzdlAUr28sHt6OsLR19dxjKyBDdOuCRL0l2lM/+k1aWlozp1z/PwsbrkU\n/QzVnj6N/tAhdIcPo0tOdr7H2Tq4/D+MwYDWxweLvz/2oCBsQUHYg4KwBwdjCwqisHt31MuOlbia\nxvQ3r4jm/Hn0+/ah37/f0RV46BDa1FRHMF9BLe5muOzfAXq9s/9ZubwFXVCAoqqObgqdznmPVouq\n1Tpa3ZeHttGI6umJrXlzbK1bY23Z0nHfqpXj31I9Hm9ypb93VTSYg7GiauxFgVwR5eJF9EeOoDt0\nCN2pUyV/Ahf9h9ZarWjOnkV/+DCatLQSfYCqRkNhjx4UDBpEweDBWMLDXesM35wc9Pv3ozt2zHFL\nTET3xx+O/lxVvXTeQ9G96u2NPSQEW7Nm2EJCsF13HfaQEOy+vo4WaG7upS/dvDw0OTlozGY0GRmX\nbmYzyoULjsAzGByhW/xTvahVikbj+Dmv1Toea7WOkDUYnPfF6yrFdShqyKiKgrV9eyz9+mELDcUW\nEoK9aVNszZo5yh4cTFDz5pX/j1/8GciAAFEOadE3cCXqraooFy+iOX8ebUoKxl27MPz0E4aEBBSb\nDbuHB5Z+/Sjs0sUZcrbrrnM8Dgqqn1EIZcnPR3v2LNqUFLRnz6K57LH27Fm0Z86gTUlxrq5qtVjb\ntMEaFobq6+tYqCglAk7Jzr60vdRU50G5sqiKgurvj91kwm4yYSu6V/39HSsU9zcXH3yzWEp0i2C3\nO/Zhs5U8SFfUL0xhIarRSGG3bhR2705hjx4Udu3qOABXDvm37vpUFfLyFMxmhaAgE0ajtOhFsaJg\nsvn7YwsLc4wOevRRlKwsDD//jHH7dow//YRxx46rjv6rWq2j5di06aX7Zs2wBQej+vo6WrNpaY4+\n47Q0tGlpKJmZjp/7vr6XDoAV9ZfamjQpuZ2mTR0BqaqObZ09e6nf+dy5SyGekoImJQWt2XxV9eze\n3o4WbkgIBQMGYOjWjYvNm2Nt3x5rmzZgMFT+s7LZ0KSnoz17FuXixUv9vp6eJR67Yp+vKJ3V6rip\nqoLdTombojjaQY4fbCpa7aV2UfH7rFYFxyEQhYICyMvTkJOjkJvruBU/zs7WkJ3teF78ODtbITNT\nw4ULl24FBY5Gyt1321i1qnbrLkHvAlRfXwpuvpmCm292LCg6yKc9e9YRqsW3ooN92tOn0e/ZgyY9\nHeWyH3SqVuto3QYFYW/SBFtIiKMrKTsbzdmz6BITHY+zslBKuWq9ajReau1ewdakiePXRWgolj59\nnL80bCEh2IvunS31IkFBQeRXt4VX9MVmb9q0eu8XDVp+PmRmasjIuPpmNl+6L75lZGjIzq7bL3Wj\nUcXHx44SdO4FAAAcGUlEQVSPj4q3t4q/v5127awEBNgJCLATGKgSEGCnT5/yf+HVBAl6V6TRYA8O\nxh4cDN26lb2e1epoxWdnYzOZHAfjKtPCVVVH4J87hzY11dGVVPRY1Wod4d2sWYl+5yq1xoXLs9sp\nav1eavWqqkJSkgdm86VWr6MVrDgfFz/Pzy/736mvrx2TyU5goOO+XTsrgYGO53q945+4RqOiKI7H\niuLoSrHZwG4vbrU7HgPodGrR8W0Vvd5xbzCAl5eKl5cdLy9HkBffe3s7wl2vr9xnERTkRW33WEnQ\nuzOdznGwsqrvUxRH942vL7ZGcjKcqL4LFxTOndNy4YKmKIQV5+O8PEcYFh8uKQ5Oux2ysxWysjRk\nZSlcvOi4z8q61K2hqqUdQL50Fr23t6Pl6+/vaA23bWvF399OQIDjub+/nSZNHGFefAsIsEubohQS\n9EK4ueIDgxkZGv78U0tiop5jx3QcO6YjMVHH+fOlH8TX6Ryt2OIBUHb7pcFQigK+vio+Pip+fnZ8\nfVVCQmxFyxzPfX3teHur+Po6WsGtW/uhKOaicJfArkkS9EI0QoWFOA/uZWcrWCwK+flgsSjOW0EB\nZGc7DhheeaCwuHukuA+7+MBgMX9/O2FhVoYNK6B9+0KaN7cVdX+oBAY6Ws7e3mqNjuoMClJJS7v6\n+I64dhL0QjQwNhskJWn5808df/6p4/hxx+PUVC2ZmY5uk5ycqh1Y1OkcrWsvL0dABwbaadXKSo8e\nxf3Zl5a1b28lONguQ/NdiAS9EPUsJUXDL78Y+d//DMTHG/jzTx0WSzPn6z4+dq6/3kqLFla6dFGd\nozYcN0e3h9Go4pgVQcVgcDw2Gh2veXs7Dh5KcLsvCXohapiqOrpW8vIU8vIcXSaXP87PV0hP1xIf\nb+CXXwycOOH4b+jjYyciwsLo0Rquuy6Ltm2tXH+9tK7FtZOgF6IMqgpnz2r44w8df/zh6Ea5cEHj\nDO28PEdoX/68OMxttoqTOSDATr9+Bfz1rzn062ehc+dCdLriM0Rz66CGwl1I0Au3lp2tkJys5fTp\nS7fkZEef+B9/6Er0hXt5OYbzeXqqeHqqeHg4hvmFhDgee3mpztfKelx88/W107q1TU7MFXVCgl40\nehcuKOza5ejbPndOy/nzGlJTtaSmajh/XsvFiwrFkzpefnq7zQZZWSWTVq9Xad7cxvXXW4mIyKVt\nWyvt2jlu110nXSiicZKgF41OerqG//3PUHQzcviwznnyjYeHnWbN7AQH22nf3srAgRZ8fe1FZz4q\nxXOSOec3ue46O6GhVkJDbbRoYSM42N6g5n4ToiZI0IsGLS9P4bffdOzfb2DfPj379ulJTHScW+7h\nYSc8vJBHH82if39HH7evb82O7RbCFUjQiwbDaoUjR3Ts2WNg7149v/2m4/DhEOeBzaZNbXTvXsj4\n8Xn061dAjx6FcvakEJUgQS/qTUqKhoQEA3v2GNizx9Faz8tz9JkHBtqIiIDhw7Pp2dNC9+6FhIRU\neVYeIQQS9KKO5OfDb7/p2b3bQEKCgd27DaSkODrDDQaVLl0KmTQpl169CunVy0Lr1jaCg4NIS8uq\n55IL0fhJ0ItqU1XHiBfHkEQdycla0tJKn172zBkthYWOLpgWLazceGMBvXs7Qr1r10KXugKiEA2N\nBL2oFFWF33/XsX27kZ9/NnD8uCPYr5xzRatVnVPLBgTYCQpyTI41ZozNGezNmkkXjBB1SYJelEpV\nITlZy44dBrZvN7Jjh5HUVEdXS+vWVjp2LGTQoALnsMTim8kkY82FaGgqDPq0tDTWrl3LhQsXUBSF\nqKgoRo0aRXZ2NqtXr+b8+fMEBwczd+5cfHx8ANiyZQuxsbFoNBqmTp1Kz549a70ioupycxWSkrQk\nJWk5eVJX4v7UKR35+Y7EDg62MWhQQdHNQosW5V90WwjRsFQY9Fqtlvvuu4+2bduSl5fHwoUL6d69\nOz/++CPdunVj7NixREdHEx0dzeTJk0lOTmbnzp2sWrUKs9nMkiVLePXVV9HIud51Ji+PEnONF889\nfvaslpMntSQlOcK8uIVezHHxB5tzHvLWra307Wvhhhus0koXohGrMOgDAwMJDAwEwNPTk9DQUDIy\nMoiPj2fx4sUAREZGsnjxYiZPnkx8fDwDBgxAr9fTtGlTQkJCSExMpEOHDrVaEXeWmqph2zYjcXFG\ntm83XhXgxRTFcXp/q1Y2hg/Pp1UrG61bW2nZ0kabNo4LS0igC+F6qtRHn5qayvHjxwkLCyMzM9P5\nBRAQEEBmZiYAGRkZtG/f3vkek8lERkbGVduKiYkhJiYGgKVLlxIUFFR+QXW6CtdxRaXV22aDuDiF\n777TEBOjcOCA49dScLDKsGF2una1EhgITZqomEzQpAmYTCqXrtGtAPqiW8Mlf3P3IvWuxX1UdsX8\n/HxWrlzJlClT8PLyKvGaoigoVWwKRkVFERUV5XyeVsFl0B1Tt9bypdIboMvrffKklo8+8mLTJi9S\nUrQYDCoRERaeeKKAyMh8One2ljsb4sWLdVToGiJ/c/ci9a665s2bV2q9SgW91Wpl5cqVDB48mL59\n+wLg7++P2WwmMDAQs9mMn58f4GjBp6enO9+bkZGByWQqdbuiYnl5sHmzJ//5jxc7dhhRFJWhQwtY\ntCiT4cML8PJS67uIQogGrsIjpKqqsm7dOkJDQxkzZoxzeXh4OHFxcQDExcURERHhXL5z504KCwtJ\nTU0lJSWFsLCwWiq+a8rLg+++MzJnTgCtW+uZNSuQU6e0zJ9/kV9+OcfGjRncemu+hLwQolIqbNH/\n/vvvbNu2jVatWjF//nwAJk6cyNixY1m9ejWxsbHO4ZUALVu2pH///sybNw+NRsP06dNlxE0lZGcr\nfP+9ka++8iQ21khurgZ/fzu33mrnttvM9O9vkYtUCCGqRVFVtUE0C8+cOVPu667af2c2Kyxb5sdH\nH3lRUKAQHGzjllvyGTUqn/79C7juOtesd2W46t+8IlJv99Jg+uhFzbPZ4MMPvXj5ZV8yMzVMnJjL\n+PF59OljkQtfCCFqlAR9Pfj1Vz1PPeXPgQMG+vUrYMmSTDp3ttZ3sYQQLkqCvg6dP6/hxRf92LTJ\ni5AQG2vXmrn99jw5SUkIUask6OtAbq7CW2958+abPlgsCjNmZDF7djY+Pg3i8IgQwsVJ0NciqxU+\n+siLlSt9OXdOy1/+ksfChRcJC5NJwYQQdUeCvhaoKmzdauTFF/04dkxPeLiFt97KICKisL6LJoRw\nQxL0NcxsVpgxI5Bt2zxo29bKO+9kcMst+dIPL4SoNxL0NejMGQ2TJzfh+HEdzz9/gcmTc9E37HnD\nhBBuQIK+hhw7pmPSJBMXL2rYuDGdgQMt9V0kIYQAJOhrxO7deu6/vwl6vcp//5tG164yJl4I0XDI\n7CnX6PvvjUyY0ISAADvR0RLyQoiGR4L+Gnz8sSdTp5oIC7MSHZ1GmzYybFII0fBI0FfTV195MGdO\nIP36Wfjkk3SCg+31XSQhhCiV9NFXw+HDOmbPDqBXLwvvv5+Oh0d9l0gIIcomLfoqMpsVpk834eur\n8s47GRLyQogGT1r0VWC1wv/9n4mUFC2ffJJGSIh01wghGj4J+ip44QU/fvrJyMqVZvr0kekMhBCN\ng3TdVNInn3jy9ts+TJ2azT335NV3cYQQotIk6Cth3z49jz8eQP/+BSxadLG+iyOEEFUiQV+B8+c1\nTJ9uIjjYxltvmWXuGiFEoyN99OWw22HWrEDMZg2ffnqeJk3k4KsQovGRoC/H66/78NNPRpYtuyBT\nGwghGi3puilDfLyBFSt8ue22PCZNyq3v4gghRLVV2KJ/4403SEhIwN/fn5UrVwKwadMmvv/+e/z8\n/ACYOHEivXv3BmDLli3Exsai0WiYOnUqPXv2rMXi1w7HxUMCaNnSxrJlF+SiIUKIRq3CoB86dCi3\n3HILa9euLbF89OjR3HbbbSWWJScns3PnTlatWoXZbGbJkiW8+uqraDSN54eDqsK8eQGcP6/l00/T\n8PWVC3gLIRq3ChO4c+fO+Pj4VGpj8fHxDBgwAL1eT9OmTQkJCSExMfGaC1mX3n3Xm+++8+Sppy7S\no4ecFCWEaPyqfTD2m2++Ydu2bbRt25b7778fHx8fMjIyaN++vXMdk8lERkZGqe+PiYkhJiYGgKVL\nlxIUFFR+QXW6Cte5VgkJCs8/r2PMGDsLFniiKJ61ur/KqIt6N1TuWnept3upi3pXK+hvvvlmxo8f\nD8BHH33E+++/z4wZM6q0jaioKKKiopzP09LSyl0/KCiownWuRVaWwsSJwQQF2XnppVTS0xtGl01t\n17shc9e6S73dy7XUu3nz5pVar1qd5wEBAWg0GjQaDcOHD+ePP/4AHC349PR053oZGRmYTKbq7KLO\nLV/uy6lTWt54w4zJ1DBCXgghakK1gt5sNjsf79q1i5YtWwIQHh7Ozp07KSwsJDU1lZSUFMLCwmqm\npLUoK0vhP//x4o478rjxRrmotxDCtVTYdfPKK69w6NAhsrKyeOihh5gwYQIHDx7kxIkTKIpCcHAw\nf//73wFo2bIl/fv3Z968eWg0GqZPn94oRtx8/LEXOTkapk3Lqe+iCCFEjVNUVW0Q/RRnzpwp9/Xa\n6r+z2yEysikBAXY+/7zh9Q+6a78luG/dpd7upcH20buSbduM/PmnjqlTpTUvhHBNbh/069d7Exxs\nY8wYmWNeCOGa3DroT57U8v33RiZPzsVgqO/SCCFE7XDroN+wwRutFiZPlm4bIYTrctugz81V+Ogj\nL0aNypeLfAshXJrbBv3mzZ5kZsqQSiGE63PLoFdVx0HYLl0KCQ+XE6SEEK7NLYP+558NHDmiZ9q0\nbJlrXgjh8twy6Nev9yYgwM7tt8uQSiGE63O7oD99WsO333owaVIOnvU/C7EQQtQ6twv699/3RlXh\nr3+V68AKIdyD2wX9F194MnRoAS1a2Oq7KEIIUSfcKugzMjScOKGjf38ZaSOEcB9uFfS7d+sB6N1b\ngl4I4T7cLOgN6HSqXPRbCOFW3C7oO3cuxNOzQUzBL4QQdcJtgt5mg7179fTuLa15IYR7cZugP3JE\nR26uhj59pH9eCOFe3Cbod+92TDgvQS+EcDduE/QJCQaaNLHRqpWMnxdCuBe3Cfrduw306WORScyE\nEG7HLYI+I0Phzz919OkjB2KFEO7HLYJ+zx7pnxdCuC9dRSu88cYbJCQk4O/vz8qVKwHIzs5m9erV\nnD9/nuDgYObOnYuPjw8AW7ZsITY2Fo1Gw9SpU+nZs2ft1qASdu82oNXKiVJCCPdUYYt+6NChPPHE\nEyWWRUdH061bN9asWUO3bt2Ijo4GIDk5mZ07d7Jq1SqefPJJ3n33Xez2+r8e6+7dBjp1KsTLS06U\nEkK4nwqDvnPnzs7WerH4+HgiIyMBiIyMJD4+3rl8wIAB6PV6mjZtSkhICImJibVQ7Mqz2WDPHr30\nzwsh3FaFXTelyczMJDAwEICAgAAyMzMByMjIoH379s71TCYTGRkZpW4jJiaGmJgYAJYuXUpQUFD5\nBdXpKlynNL/9ppCToyEy0lit99e36tbbFbhr3aXe7qUu6l2toL+coigo1RizGBUVRVRUlPN5Wlpa\nuesHBQVVuE5pYmK8gAA6dEgnLa3xjaGvbr1dgbvWXertXq6l3s2bN6/UetUadePv74/ZbAbAbDbj\n5+cHOFrw6enpzvUyMjIwmUzV2UWN2b3bgMlko02bxhfyQghRE6oV9OHh4cTFxQEQFxdHRESEc/nO\nnTspLCwkNTWVlJQUwsLCaq601ZCQ4JjITE6UEkK4qwq7bl555RUOHTpEVlYWDz30EBMmTGDs2LGs\nXr2a2NhY5/BKgJYtW9K/f3/mzZuHRqNh+vTpaDT1N1TfbFZITNRz55159VYGIYSobxUG/Zw5c0pd\n/swzz5S6fNy4cYwbN+7aSlVD5EQpIYRw8TNjExIMaDQqPXvK0EohhPty6aDfvVtPx45WvL3lRCkh\nhPty2aC32x1dN9JtI4Rwdy4b9EeP6sjK0tC7twS9EMK9uWzQJyTIgVghhAAXDvrdu/UEBtpo21ZO\nlBJCuDcXDnqDnCglhBC4aNDn58OxY3qZf14IIXDRoE9OdpwH1qaNtZ5LIoQQ9c8lg/7UKS0ALVtK\n/7wQQrhk0CclFQe9tOiFEMIlg/7UKR1Go0qzZvV/GUMhhKhvLhn0SUlaQkNt1OPEmUII0WC4ZBQm\nJ2ul20YIIYq4ZNAnJWnlQKwQQhRxuaDPzlYwm7W0aiVBL4QQ4IJBLyNuhBCiJJcL+uRkGUMvhBCX\nc7mgT0pynBUrXTdCCOHggkGvxcvLjskkY+iFEAJcMOgdQyttMmulEEIUcbmgT0rSSf+8EEJcRnct\nb545cyYeHh5oNBq0Wi1Lly4lOzub1atXc/78eYKDg5k7dy4+Pj41Vd5yqapjQrP+/QvqZH9CCNEY\nXFPQAyxatAg/Pz/n8+joaLp168bYsWOJjo4mOjqayZMnX+tuKsVsVsjO1kiLXgghLlPjXTfx8fFE\nRkYCEBkZSXx8fE3vokzF89BL0AshxCXX3KJfsmQJGo2GESNGEBUVRWZmJoGBgQAEBASQmZl5zYWs\nLDlZSgghrnZNQb9kyRJMJhOZmZk8//zzNG/evMTriqKglDH8JSYmhpiYGACWLl1KUFBQ+QXV6Spc\nJz3d8QOlV68A/P0rW4uGrTL1dlXuWnept3upi3pfU9CbTCYA/P39iYiIIDExEX9/f8xmM4GBgZjN\n5hL995eLiooiKirK+TwtLa3cfQUFBVW4zpEj/gQEaCgsTKOCVRuNytTbVblr3aXe7uVa6n1l47os\n1e6jz8/PJy8vz/l4//79tGrVivDwcOLi4gCIi4sjIiKiuruosuRkLS1aSLeNEEJcrtot+szMTFas\nWAGAzWZj0KBB9OzZk3bt2rF69WpiY2OdwyvrSlKSlg4dJOiFEOJy1Q76Zs2asXz58quW+/r68swz\nz1xToapDVR2jboYPlzH0QghxOZc5M/b8eQ35+QqtWkmLXgghLucyQV88tLJFCxlDL4QQl3OZoD91\nSqYnFkKI0rhM0F86WUqCXgghLucyQZ+crCUoyIanp1rfRRFCiAbFZYJepicWQojSuUzQnzqllRE3\nQghRCpcIepsNTp/WSoteCCFK4RJBf/asFqtVkaAXQohSuETQF4+4kaGVQghxNZcKepmHXgghruYS\nQZ+crENRVEJDpUUvhBBXcomgT0rSEhJix2Co75IIIUTD4xJBL0MrhRCibC4R9HKylBBClK3RB73F\nAmfPaiTohRCiDI0+6E+f1qKqioy4EUKIMjT6oJfpiYUQonyNPujlZCkhhChfow/6U6e06HQqISES\n9EIIURqXCPrQUBtabX2XRAghGqZGH/QytFIIIcrX6INeTpYSQojy6Wprw3v37mX9+vXY7XaGDx/O\n2LFja3wfeXkKaWlaWrSQFr0QQpSlVlr0drudd999lyeeeILVq1ezY8cOkpOTa3w/p07JiBshhKhI\nrQR9YmIiISEhNGvWDJ1Ox4ABA4iPj6/x/Wg0MGZMHjfcUFjj2xZCCFdRK103GRkZNGnSxPm8SZMm\nHDt2rMb3ExZm5a23zDW+XSGEcCW11kdfkZiYGGJiYgBYunQpQUFB5a6v0+kqXMcVuWu9wX3rLvV2\nL3VR71oJepPJRHp6uvN5eno6JpOpxDpRUVFERUU5n6elpZW7zaCgoArXcUXuWm9w37pLvd3LtdS7\nefPmlVqvVvro27VrR0pKCqmpqVitVnbu3El4eHht7EoIIUQFaqVFr9VqmTZtGi+88AJ2u52bbrqJ\nli1b1sauhBBCVKDW+uh79+5N7969a2vzQgghKqnRnxkrhBCifBL0Qgjh4iTohRDCxSmqqqr1XQgh\nhBC1p9G06BcuXFjfRagX7lpvcN+6S73dS13Uu9EEvRBCiOqRoBdCCBenXbx48eL6LkRltW3btr6L\nUC/ctd7gvnWXeruX2q63HIwVQggXJ103Qgjh4iTohRDCxdXbfPRVURfXn20I3njjDRISEvD392fl\nypUAZGdns3r1as6fP09wcDBz587Fx8ennktas9LS0li7di0XLlxAURSioqIYNWqUy9fdYrGwaNEi\nrFYrNpuNfv36MWHCBJevdzG73c7ChQsxmUwsXLjQLeo9c+ZMPDw80Gg0aLVali5dWjf1Vhs4m82m\nPvzww+rZs2fVwsJC9bHHHlNPnTpV38WqFQcPHlT/+OMPdd68ec5lH3zwgbplyxZVVVV1y5Yt6gcf\nfFBfxas1GRkZ6h9//KGqqqrm5uaqjzzyiHrq1CmXr7vdblfz8vJUVVXVwsJC9R//+If6+++/u3y9\ni33++efqK6+8or700kuqqrrHv/UZM2aomZmZJZbVRb0bfNdNXV1/tiHo3LnzVd/k8fHxREZGAhAZ\nGemSdQ8MDHSOOvD09CQ0NJSMjAyXr7uiKHh4eABgs9mw2WwoiuLy9QbHxYgSEhIYPny4c5k71Ls0\ndVHvBt91U1fXn22oMjMzCQwMBCAgIIDMzMx6LlHtSk1N5fjx44SFhblF3e12OwsWLODs2bOMHDmS\n9u3bu0W9N2zYwOTJk8nLy3Muc4d6AyxZsgSNRsOIESOIioqqk3o3+KAXlyiKgqIo9V2MWpOfn8/K\nlSuZMmUKXl5eJV5z1bprNBqWL19OTk4OK1asICkpqcTrrljv3bt34+/vT9u2bTl48GCp67hivcER\n8iaTiczMTJ5//vmrLgVYW/Vu8EFfmevPujJ/f3/MZjOBgYGYzWb8/Pzqu0i1wmq1snLlSgYPHkzf\nvn0B96k7gLe3N126dGHv3r0uX+/ff/+dX3/9lT179mCxWMjLy2PNmjUuX2/AmV3+/v5ERESQmJhY\nJ/Vu8H307n792fDwcOLi4gCIi4sjIiKinktU81RVZd26dYSGhjJmzBjnclev+8WLF8nJyQEcI3D2\n799PaGioy9d70qRJrFu3jrVr1zJnzhy6du3KI4884vL1zs/Pd3ZV5efns3//flq1alUn9W4UZ8Ym\nJCTw3nvvOa8/O27cuPouUq145ZVXOHToEFlZWfj7+zNhwgQiIiJYvXo1aWlpLjvk7MiRIzzzzDO0\natXK+bN14sSJtG/f3qXrfvLkSdauXYvdbkdVVfr378/48ePJyspy6Xpf7uDBg3z++ecsXLjQ5et9\n7tw5VqxYATgOvg8aNIhx48bVSb0bRdALIYSovgbfdSOEEOLaSNALIYSLk6AXQggXJ0EvhBAuToJe\nCCFcnAS9EKWYOXMm+/fvr+9iCFEjJOiFEMLFSdALIYSLk6AXogLJycnMnDmT7du313dRhKiWBj+p\nmRD16c8//2T58uU88MAD9OnTp76LI0S1SNALUYYjR44QGxvLrFmz6NKlS30XR4hqk64bIcqwdetW\nOnToICEvGj0JeiHK8Le//Y309HQ2bNhQ30UR4ppI0AtRBg8PD5544gkOHz7Mv//97/oujhDVJkEv\nRDm8vb15+umn2bt3L//5z3/quzhCVIvMRy+EEC5OWvRCCOHiJOiFEMLFSdALIYSLk6AXQggXJ0Ev\nhBAuToJeCCFcnAS9EEK4OAl6IYRwcf8fLwptJbnbq48AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1126569d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LOOs = []\n",
    "MSEs = []\n",
    "K=50\n",
    "Ks = range(1,K+1)\n",
    "for k in Ks:\n",
    "    knn = neighbors.KNeighborsRegressor(n_neighbors=k)\n",
    "    LOOs.append(loo_risk(X,y,knn))\n",
    "    MSEs.append(emp_risk(X,y,knn))\n",
    "\n",
    "plt.plot(Ks,LOOs,'r',label=\"LOO risk\")\n",
    "plt.title(\"Risks for kNN Regression\")\n",
    "plt.plot(Ks,MSEs,'b',label=\"Emp risk\")\n",
    "plt.legend()\n",
    "_ = plt.xlabel('k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the risks for KNN regression indicate the variance dominates for smaller k and after k is greater than 20, LOO risk turns to be stable.LOO risk is always larger than empirical risk. The performance is somewhat better for k around 15. This demonstrates that Empirical risk can be trustable especially after k is greater than 40. The KNN regression outperms the linear regression, compared the LOO risk of KNN regression with that of linear regression(LOO=243.53350804)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 2.3__ (10 pts) Implement forward stepwise regression (ESL section 3.3.2) for the linear model and compare the LOO risk for each stage.  Recall that at each step forward stepwise regression will select a new variable that most improves the empirical risk and include that in the model (starting with the intercept)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forwardstep(X,y,regmod):\n",
    "    \"\"\"\n",
    "    Return LOO and number of variables in the regression model based on\n",
    "            forward stepwise regression \n",
    "    Input: design matrix, X, response vector, y, a regression model, regmod\n",
    "    Output: leave-one-out risk;  and number of X features\n",
    "    \"\"\"\n",
    "    LOOs=[]\n",
    "    x1=[] # start from intercept\n",
    "    n,p=X.shape\n",
    "    x2=range(0,p) # potential condidate X\n",
    "    while len(x2)>0:\n",
    "        mse=[]\n",
    "        for i in x2:\n",
    "            x1_t=X[:,x1+[i]]\n",
    "            mse.append(emp_risk(x1_t,y,regmod))\n",
    "        x1.append(x2[mse.index(min(mse))])\n",
    "        #x2=x2.drop(x2.columns([mse_temp.index(min(mse_temp))]),axis=1)\n",
    "        del x2[mse.index(min(mse))]\n",
    "        LOOs.append(loo_risk(X[:,x1],y,regmod))\n",
    "    results=np.concatenate((LOOs,x1),axis=0)\n",
    "    results=np.reshape(results,(2,p))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.85322318,  0.80952469,  0.80266804,  0.80373347],\n",
       "       [ 3.        ,  1.        ,  2.        ,  0.        ]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import scale \n",
    "xs=scale(X)\n",
    "ys=scale(y)\n",
    "model = linear_model.LinearRegression(fit_intercept=False)\n",
    "forwardstep(xs,ys,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 303.59878287,  258.68779755,  245.93873129,  243.53350804],\n",
       "       [   0.        ,    3.        ,    1.        ,    2.        ]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forwardstep(X,y,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LOO risk through each step indicates that the scale of the variables directly relate to the final selection. For unscaled dataset,after the first X density is included, LOO is 303.59878; then time,pH and alcohol are selected in the model step by step. The corresponding LOO is 258.68779,245.93873 and 243.5335 respectively."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
